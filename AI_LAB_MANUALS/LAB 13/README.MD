Lab No. 13
Natural Language Processing (NLP)

Questions:
1.	What is the core idea behind Word2Vec?
2.	Difference between CBOW and Skip-Gram?
3.	Why is one-hot encoding inefficient?
4.	Why do character names appear close in vector space?
5.	How does window size affect semantic learning?
6.	Why might rare characters have poor embeddings?
7.	Which model performed better: CBOW or Skip-Gram? Why?
8.	What happens if vector size is too small or too large?
9.	Can Word2Vec understand word meaning without labels? Explain.
